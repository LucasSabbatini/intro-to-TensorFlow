{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, world!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello_constant = tf.constant('Hello, world!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\toutput = sess.run(hello_constant)\n",
    "\tprint(output)\n",
    "\n",
    "# Tensor is the object in which data is stored in tensorflow, and they come in a variaety of sizes\n",
    "\n",
    "A = tf.constant(1234)\n",
    "# A is a 0 dimensional int32 tensor\n",
    "\n",
    "B = tf.constant([123, 456, 789])\n",
    "# B is a 1 dimensional int32 tensor\n",
    "\n",
    "C = tf.constant([ [123], [456] ])\n",
    "# C is a 2 dimensional int32 tensor\n",
    "\n",
    "# tensor tf.constant() is called constant because its vallue never changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph\n",
    "\n",
    "<img src='tensorflowgraph.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Session is an environment for running a graph. the code has already created a tensor, from previous lines, the next step is to evaluate the tensor in a session.\n",
    "\n",
    "The code created a session instance, sess, using tf.Session(). The sess.run() function then evaluates the tensor and returns the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input\n",
    "\n",
    "tf.placeholder(): it isn't a good idea to set your input definetelly, since you'll want your tensorflow model to take in different datasets with different parameters. the values will be passed in to the tensor using the tf.session.run() function, inside a session, so the tensor values can be set right before the session runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test String\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y:123.4, z:45.67})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'y' of 'Sub' Op has type int32 that does not match type float32 of argument 'x'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    492\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[1;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         % (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[0;32m    590\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(\"Const_5:0\", shape=(), dtype=int32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-70a3f214da5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Fails with ValueError: Tensor conversion requested\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[0msubtract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`Sub`\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"`tf.subtract`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_sub\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   2773\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2774\u001b[0m   \"\"\"\n\u001b[1;32m-> 2775\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sub\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2776\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    520\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[1;32m--> 522\u001b[1;33m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'y' of 'Sub' Op has type int32 that does not match type float32 of argument 'x'."
     ]
    }
   ],
   "source": [
    "x = tf.subtract(tf.constant(2.0), tf.constant(1)) # Fails with ValueError: Tensor conversion requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))\n",
    "print(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x)\n",
    "    \n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Quiz: Tensorflow Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = 10\n",
    "y = 2\n",
    "z = x/y - 1\n",
    "\n",
    "# we set values for constants and operations outside the session\n",
    "x = tf.cast(tf.constant(x), tf.float32)\n",
    "y = tf.cast(tf.constant(y), tf.float32)\n",
    "z = tf.subtract(tf.cast(tf.divide(x, y), tf.int32), tf.constant(1))\n",
    "# but they'll only be performed within a session\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    # operations are run within the session\n",
    "    # tf.placeholder() and tf.constant() are set outside the session\n",
    "    output = sess.run(z)\n",
    "    print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quiz:TensorFlow Linear Function\n",
    "\n",
    "Since we need to update weights and biases, we cannot use tf.placeholder() of td.constant(), so we'll use tf.Variable() for them. Since x will not be updated to the learning process, only set again when a new data point is being analysed, tf.placeholder may be used.\n",
    "\n",
    "This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the tf.global_variables_initializer() function to initialize the state of all the Variable tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(5) \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.\n",
    "\n",
    "Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You'll use the tf.truncated_normal() function to generate random numbers from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "# (n_feaures, n_labels) means there's no hidden layer, it goes straight to the output       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. The simplest solution is to set them to 0 with tf.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_labels = 5 \n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "# the bias here isnt a 2d matrix 1 x n_labels, it is alradey a vector of len= n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(n_features, n_labels):\n",
    "    return tf.Variable(tf.truncated_normal((n_feature, n_leabels)))\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "def linear(inp, w, b):\n",
    "    return tf.add(tf.matmul(inp, w), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tha Sigmoid function is useful when the output (prediction) is binary, wither one or zero.\n",
    "\n",
    "<img src='sigmoidfunction.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you need to classify outputs between more than two classes, the sigmoid function isn't' usefull anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations Functions So Far\n",
    "\n",
    "Softmax: squashes input values between 0 and 1. Useful for binary classification problems. since the derivative of this functions maxes out at 0.25, networks with a lot of layers will have its gradients of layers close to input layer vanished. GRADIENT VANISHING PROBLEM. USEFUL FOR BINARY CLASSIFICATION\n",
    "\n",
    "Rectfied Linear Units: F(x)=max(x, 0). Is a singularity function, and does not have the problem of venishing gradients, but another problem comes up. If we update weights in a way such that the input of the softmax function becomes zero, the node will die.\n",
    "\n",
    "Softmax: Softmax activation function outputs a categorical classification probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.nn.softmax([2.0, 1.0, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding with Scikit-Learn\n",
    "\n",
    "Transforming your labels into one-hot encode vectors is pretty simple with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "labels = np.array([1, 5, 3, 2, 1, 4, 2, 1, 3])\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "lb.fit(labels)\n",
    "\n",
    "lb.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy\n",
    "\n",
    "So, at the end of a forward pass you're going to have two vectors, the prediction made by the network and the tru label one-hot encoded vector. To get the error for this, you're not going to calculate the error for every class in those vectors, but only for the tru label. \n",
    "\n",
    "<img src='cross-entropy-diagram.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "cross_entropy = tf.reduce_sum(-tf.multiply(one_hot, tf.log(softmax)))\n",
    "feed_dict = {one_hot:one_hot_data, softmax:softmax_data}\n",
    "# open session\n",
    "with tf.Session() as sess:\n",
    "    cross = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "    print(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE CANNOT UPDATE OUR MODEL AFTER WE'VE SEEN HOW IT PERFOMRED IN A TEST SET.\n",
    "\n",
    "If we've trained our model, tested in a test set, and then update our model to better fit this test set, your model will be biased towards this data, and when it finally finds real world data, it may not perform as well. That's why you need to split your data into three sets: training, validation and testing. Now you can train your model with the training set, test it with the validation set, adjust parameters of your network, and when you're performing good in both test and validation sets, you're good to measure the performance in your test set. To have a real measure of how well your model fit unseen data, your model must not see the test data while training and validating.\n",
    "\n",
    "If you've seen your model performence in a test, and used that information to make a decision about your classifier, it means your model have seen the test set indirectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Data and Initializing Weights and Biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainging logist regression with grasient descent is great, you're directly optimizing the error measure. But there's a problem, which is it is very difficult to scale.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Instead of computing the average of the loss over every data point (which id inefficient since we usually train the models over huge data sets), we're going to take an estimate of the error, taking the error of a random sample of the data. We're going to take a very small sliver of the training data, compute the loss for that sample, compute the derivative for that sample, and pretend that that derivative is the right direction to use for gradient descent. It is not the right direction at all, but theres a way to compensate for it by doing it many times and taking smaller steps. \n",
    "\n",
    "Stochastic Gradient Descent is at the core of deep learning, beacuse it scales well with both data and model size, an we want both: big data and big models.\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum and Learning Rate Decay\n",
    "\n",
    "If we calculated the gradient over all data points in the data, the step would be in the directoin of maximum optimization, which is direction of minimum loss. But we're not actually doing that. Since we're not taking the actual gradient of the whole training set (we're stochastically taking samples and using the gradient of that sample), each step will be taken to a somewhat random direction, but on aggregate, those steps take us to the minimum of the loss. \n",
    "\n",
    "We can take advantage of the knowledge we've taken from previous steps. We can do that by keep a running average of the gradients, and instead of taking a step in the direction of the current calculated gradient (over random btach), we take a step towards this average gradient\n",
    "\n",
    "M = 0.9M + L\n",
    "\n",
    "Another technique used to optimize the training process is the learning rate decay, which consists on loweriing the learning rate of the regression over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Hyperspace\n",
    "\n",
    "Hyper-Parameters:\n",
    "\n",
    "-Initial Learning Rate\n",
    "\n",
    "-Learning Rate Decay\n",
    "\n",
    "-Momentum\n",
    "\n",
    "-Batch Size\n",
    "\n",
    "-Weight Initialization\n",
    "\n",
    "\n",
    "WHEN THINGS DON'T WORK, ALWAYS TRY LOWERING YOUR LEARNING RATE FIRST!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAGRAD\n",
    "\n",
    "Implements initial learning rate, learning rate decay and momentum for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batching\n",
    "\n",
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's tf.placeholder() function to receive the varying batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /datasets/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /datasets/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /datasets/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /datasets/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5e6ad5d7972f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# wegihts and bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_classes' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784\n",
    "n_classees = 10\n",
    "\n",
    "mnist = input_data.read_data_sets('/datasets/', one_hot=True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "# the features are already scaled and the data is shuffled\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# wegihts and bias\n",
    "weights = tf.Variable(tf.truncated_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Model With 10 Epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 1    - Cost: 10.9     Valid Accuracy: 0.0718\n",
      "Epoch: 2    - Cost: 10.2     Valid Accuracy: 0.0812\n",
      "Epoch: 3    - Cost: 9.59     Valid Accuracy: 0.097\n",
      "Epoch: 4    - Cost: 9.1      Valid Accuracy: 0.116\n",
      "Epoch: 5    - Cost: 8.67     Valid Accuracy: 0.135\n",
      "Epoch: 6    - Cost: 8.28     Valid Accuracy: 0.157\n",
      "Epoch: 7    - Cost: 7.91     Valid Accuracy: 0.179\n",
      "Epoch: 8    - Cost: 7.56     Valid Accuracy: 0.2  \n",
      "Epoch: 9    - Cost: 7.24     Valid Accuracy: 0.218\n",
      "Epoch: 10   - Cost: 6.93     Valid Accuracy: 0.237\n",
      "Epoch: 11   - Cost: 6.64     Valid Accuracy: 0.257\n",
      "Epoch: 12   - Cost: 6.37     Valid Accuracy: 0.278\n",
      "Epoch: 13   - Cost: 6.11     Valid Accuracy: 0.297\n",
      "Epoch: 14   - Cost: 5.88     Valid Accuracy: 0.314\n",
      "Epoch: 15   - Cost: 5.65     Valid Accuracy: 0.331\n",
      "Epoch: 16   - Cost: 5.45     Valid Accuracy: 0.348\n",
      "Epoch: 17   - Cost: 5.25     Valid Accuracy: 0.364\n",
      "Epoch: 18   - Cost: 5.07     Valid Accuracy: 0.379\n",
      "Epoch: 19   - Cost: 4.9      Valid Accuracy: 0.392\n",
      "Epoch: 20   - Cost: 4.74     Valid Accuracy: 0.409\n",
      "Epoch: 21   - Cost: 4.6      Valid Accuracy: 0.424\n",
      "Epoch: 22   - Cost: 4.46     Valid Accuracy: 0.436\n",
      "Epoch: 23   - Cost: 4.34     Valid Accuracy: 0.447\n",
      "Epoch: 24   - Cost: 4.22     Valid Accuracy: 0.46 \n",
      "Epoch: 25   - Cost: 4.11     Valid Accuracy: 0.471\n",
      "Epoch: 26   - Cost: 4.0      Valid Accuracy: 0.481\n",
      "Epoch: 27   - Cost: 3.91     Valid Accuracy: 0.491\n",
      "Epoch: 28   - Cost: 3.81     Valid Accuracy: 0.502\n",
      "Epoch: 29   - Cost: 3.73     Valid Accuracy: 0.511\n",
      "Epoch: 30   - Cost: 3.65     Valid Accuracy: 0.521\n",
      "Epoch: 31   - Cost: 3.57     Valid Accuracy: 0.529\n",
      "Epoch: 32   - Cost: 3.49     Valid Accuracy: 0.536\n",
      "Epoch: 33   - Cost: 3.42     Valid Accuracy: 0.543\n",
      "Epoch: 34   - Cost: 3.35     Valid Accuracy: 0.55 \n",
      "Epoch: 35   - Cost: 3.29     Valid Accuracy: 0.557\n",
      "Epoch: 36   - Cost: 3.23     Valid Accuracy: 0.566\n",
      "Epoch: 37   - Cost: 3.17     Valid Accuracy: 0.572\n",
      "Epoch: 38   - Cost: 3.11     Valid Accuracy: 0.577\n",
      "Epoch: 39   - Cost: 3.06     Valid Accuracy: 0.583\n",
      "Epoch: 40   - Cost: 3.0      Valid Accuracy: 0.589\n",
      "Epoch: 41   - Cost: 2.95     Valid Accuracy: 0.592\n",
      "Epoch: 42   - Cost: 2.9      Valid Accuracy: 0.597\n",
      "Epoch: 43   - Cost: 2.85     Valid Accuracy: 0.601\n",
      "Epoch: 44   - Cost: 2.81     Valid Accuracy: 0.605\n",
      "Epoch: 45   - Cost: 2.76     Valid Accuracy: 0.61 \n",
      "Epoch: 46   - Cost: 2.72     Valid Accuracy: 0.613\n",
      "Epoch: 47   - Cost: 2.68     Valid Accuracy: 0.617\n",
      "Epoch: 48   - Cost: 2.64     Valid Accuracy: 0.622\n",
      "Epoch: 49   - Cost: 2.6      Valid Accuracy: 0.628\n",
      "Epoch: 50   - Cost: 2.56     Valid Accuracy: 0.632\n",
      "Epoch: 51   - Cost: 2.52     Valid Accuracy: 0.635\n",
      "Epoch: 52   - Cost: 2.49     Valid Accuracy: 0.638\n",
      "Epoch: 53   - Cost: 2.45     Valid Accuracy: 0.643\n",
      "Epoch: 54   - Cost: 2.42     Valid Accuracy: 0.647\n",
      "Epoch: 55   - Cost: 2.38     Valid Accuracy: 0.652\n",
      "Epoch: 56   - Cost: 2.35     Valid Accuracy: 0.654\n",
      "Epoch: 57   - Cost: 2.32     Valid Accuracy: 0.657\n",
      "Epoch: 58   - Cost: 2.29     Valid Accuracy: 0.66 \n",
      "Epoch: 59   - Cost: 2.26     Valid Accuracy: 0.662\n",
      "Epoch: 60   - Cost: 2.23     Valid Accuracy: 0.666\n",
      "Epoch: 61   - Cost: 2.2      Valid Accuracy: 0.671\n",
      "Epoch: 62   - Cost: 2.17     Valid Accuracy: 0.674\n",
      "Epoch: 63   - Cost: 2.15     Valid Accuracy: 0.676\n",
      "Epoch: 64   - Cost: 2.12     Valid Accuracy: 0.68 \n",
      "Epoch: 65   - Cost: 2.09     Valid Accuracy: 0.682\n",
      "Epoch: 66   - Cost: 2.07     Valid Accuracy: 0.684\n",
      "Epoch: 67   - Cost: 2.04     Valid Accuracy: 0.687\n",
      "Epoch: 68   - Cost: 2.02     Valid Accuracy: 0.688\n",
      "Epoch: 69   - Cost: 2.0      Valid Accuracy: 0.691\n",
      "Epoch: 70   - Cost: 1.97     Valid Accuracy: 0.693\n",
      "Epoch: 71   - Cost: 1.95     Valid Accuracy: 0.695\n",
      "Epoch: 72   - Cost: 1.93     Valid Accuracy: 0.697\n",
      "Epoch: 73   - Cost: 1.91     Valid Accuracy: 0.699\n",
      "Epoch: 74   - Cost: 1.89     Valid Accuracy: 0.7  \n",
      "Epoch: 75   - Cost: 1.87     Valid Accuracy: 0.702\n",
      "Epoch: 76   - Cost: 1.85     Valid Accuracy: 0.704\n",
      "Epoch: 77   - Cost: 1.83     Valid Accuracy: 0.705\n",
      "Epoch: 78   - Cost: 1.81     Valid Accuracy: 0.707\n",
      "Epoch: 79   - Cost: 1.79     Valid Accuracy: 0.709\n",
      "Epoch: 80   - Cost: 1.77     Valid Accuracy: 0.711\n",
      "Epoch: 81   - Cost: 1.75     Valid Accuracy: 0.712\n",
      "Epoch: 82   - Cost: 1.73     Valid Accuracy: 0.714\n",
      "Epoch: 83   - Cost: 1.71     Valid Accuracy: 0.716\n",
      "Epoch: 84   - Cost: 1.7      Valid Accuracy: 0.717\n",
      "Epoch: 85   - Cost: 1.68     Valid Accuracy: 0.72 \n",
      "Epoch: 86   - Cost: 1.66     Valid Accuracy: 0.721\n",
      "Epoch: 87   - Cost: 1.65     Valid Accuracy: 0.723\n",
      "Epoch: 88   - Cost: 1.63     Valid Accuracy: 0.724\n",
      "Epoch: 89   - Cost: 1.62     Valid Accuracy: 0.725\n",
      "Epoch: 90   - Cost: 1.6      Valid Accuracy: 0.727\n",
      "Epoch: 91   - Cost: 1.59     Valid Accuracy: 0.728\n",
      "Epoch: 92   - Cost: 1.57     Valid Accuracy: 0.73 \n",
      "Epoch: 93   - Cost: 1.56     Valid Accuracy: 0.731\n",
      "Epoch: 94   - Cost: 1.54     Valid Accuracy: 0.731\n",
      "Epoch: 95   - Cost: 1.53     Valid Accuracy: 0.732\n",
      "Epoch: 96   - Cost: 1.52     Valid Accuracy: 0.734\n",
      "Epoch: 97   - Cost: 1.5      Valid Accuracy: 0.736\n",
      "Epoch: 98   - Cost: 1.49     Valid Accuracy: 0.737\n",
      "Epoch: 99   - Cost: 1.48     Valid Accuracy: 0.739\n",
      "Epoch: 100  - Cost: 1.47     Valid Accuracy: 0.74 \n",
      "Test Accuracy: 0.7493000030517578\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n",
    "\n",
    "def print_epochs_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(cost, feed_dict={features:last_features, labels:last_labels})\n",
    "    \n",
    "    valid_accuracy = sess.run(accuracy, feed_dict={features:valid_features, labels:valid_labels})\n",
    "    \n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(epoch_i, current_cost, valid_accuracy))\n",
    "    \n",
    "\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# features are already scaled and shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "# labels\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "validation_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# laceholder for features an labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - Forward pass\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# variables initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # iterate over epochs\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        # iterate over every batch and run optimizer\n",
    "        for (batch_features, batch_labels) in train_batches:\n",
    "            sess.run(optimizer, feed_dict={features:batch_features, labels:batch_labels, learning_rate:learn_rate})\n",
    "            # when you run optimizer, every dependency will also be run, so you need to feed the placeholders needed\n",
    "            \n",
    "        print_epochs_stats(epoch, sess, batch_features, batch_labels)\n",
    "        \n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features:test_features, labels:test_labels})\n",
    "    \n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
